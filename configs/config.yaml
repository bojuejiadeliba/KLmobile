---
text_model:
  num_embeddings: 50257
  max_seq_length: 30
  embedding_dim: 256

  transformer_encoder:
    num_heads: 4
    dropout_rate: 0.3

  lconv:
    num_heads: 4
    kernel_size: [3, 5, 7, 31, 31, 31]
    padding: ["same", "same", "same", "same", "same", "same"]
    weight_softmax: [True, True, True, True, True, True]
    bias: [False, False, False, False, False, False]
    dropout_rate: [0.3, 0.3, 0.3, 0.3, 0.3, 0.3]

  n_blocks: 4
  output_dim: 768
  dropout_rate: 0.3
